{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Logistic Regression | Assignment**"
      ],
      "metadata": {
        "id": "uS2ORL8Oo-rO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 1: What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "Logistic Regression is a supervised learning algorithm primarily used for classification tasks (often binary classification). Instead of predicting continuous numerical values like Linear Regression, Logistic Regression outputs probabilities of different classes.\n",
        "-  In Linear Regression, we directly predict a continuous value. For example, predicting a house price based on features like square footage or location.\n",
        "- In Logistic Regression, we use the predicted values within the Sigmoid function to obtain a probability between 0 and 1, which is then used for classification.\n",
        "\n",
        "Mathematically, Linear Regression is:\n",
        "\n",
        "y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n",
        "\n",
        "Whereas Logistic Regression applies a Sigmoid (or logistic) function to the linear combination of features:\n",
        "\n",
        "p = 1 / (1 + e^(-z)), where z = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n",
        "\n",
        "##Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n",
        "The Sigmoid function, also known as the logistic function, maps any real-valued input into a number between 0 and 1. Specifically,\n",
        "Sigmoid(z) = 1 / (1 + e^(-z))\n",
        "- In Logistic Regression, we feed the linear combination of the input features (z) into the Sigmoid.\n",
        "- The output of the Sigmoid function can be interpreted as the probability of the instance belonging to the positive class (e.g., “Class 1”).\n",
        "- If the Sigmoid output is greater than 0.5, the instance is typically classified as Class 1; otherwise, it is classified as Class 0.\n",
        "\n",
        "##Question 3: What is Regularization in Logistic Regression and why is it needed?\n",
        "Regularization is a technique to prevent overfitting by penalizing large coefficients in the model. It forces the model to keep the coefficient values smaller, thus reducing variance and improving the model’s generalization capability.\n",
        "- In Logistic Regression, common regularization techniques include L1 (Lasso) and L2 (Ridge).\n",
        "- L2 Regularization (penalty='l2'): Adds a term proportional to the square of the magnitude of the coefficients.\n",
        "- L1 Regularization (penalty='l1'): Adds a term proportional to the absolute value of the coefficients and can drive some coefficients to exactly zero, performing feature selection.\n",
        "\n",
        "Regularization is critical to ensure the model does not memorize noise from training data and can generalize well to unseen data.\n",
        "\n",
        "##Question 4: What are some common evaluation metrics for classification models, and why are they important?\n",
        "Common classification evaluation metrics include:\n",
        "1.\tAccuracy: The proportion of correct predictions (both true positives and true negatives) among the total number of predictions.\n",
        "2.\tPrecision: The proportion of true positives among all predicted positives. Helps measure how precise the model is in correctly predicting positives.\n",
        "3.\tRecall (Sensitivity): The proportion of true positives among all actual positives. Reflects how many of the actual positives the model captures.\n",
        "4.\tF1-Score: The harmonic mean of precision and recall. It provides a single metric balancing both precision and recall, especially useful for imbalanced datasets.\n",
        "5.\tROC-AUC (Receiver Operating Characteristic - Area Under Curve): Measures how well the model separates classes at different thresholds. A higher AUC indicates better separability of classes.\n",
        "\n",
        "These metrics are important because they give deeper insights into the model’s performance beyond just accuracy, helping to identify its strengths and weaknesses in different aspects of classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "OKH0iEquqy5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy. (Use Dataset from sklearn package)\n",
        "\n",
        "Below is an example using the Iris dataset from sklearn, saved to a CSV, then reloaded"
      ],
      "metadata": {
        "id": "ijmEQvx7zLL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load a dataset from sklearn package\n",
        "breast_cancer = load_breast_cancer()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "df['target'] = breast_cancer.target\n",
        "\n",
        "# Save to CSV file\n",
        "df.to_csv('breast_cancer_data.csv', index=False)\n",
        "print(\"Dataset saved to 'breast_cancer_data.csv'\")\n",
        "\n",
        "# Step 2: Load the CSV file into a Pandas DataFrame\n",
        "data = pd.read_csv('breast_cancer_data.csv')\n",
        "print(f\"\\nDataset loaded successfully!\")\n",
        "print(f\"Shape of dataset: {data.shape}\")\n",
        "\n",
        "# Step 3: Split the data into features (X) and target (y)\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "# Step 4: Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")\n",
        "\n",
        "# Step 5: Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"\\nLogistic Regression Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Accuracy in percentage: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvzhYCzQpLhn",
        "outputId": "9d50d7f9-737b-43d5-c0af-91d4d64963d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset saved to 'breast_cancer_data.csv'\n",
            "\n",
            "Dataset loaded successfully!\n",
            "Shape of dataset: (569, 31)\n",
            "\n",
            "Training samples: 455\n",
            "Testing samples: 114\n",
            "\n",
            "Logistic Regression Model Accuracy: 0.9561\n",
            "Accuracy in percentage: 95.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 6: Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy.   (Use Dataset from sklearn package)\n"
      ],
      "metadata": {
        "id": "EXvY5hCrzfpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load the Wine dataset from sklearn\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "print(\"Wine Dataset Information:\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Number of classes: {len(np.unique(y))}\")\n",
        "print(f\"Feature names: {wine.feature_names}\")\n",
        "print(f\"Target names: {wine.target_names}\")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Scale the features (recommended for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression with L2 regularization (Ridge)\n",
        "# penalty='l2' is default, but explicitly mentioning for clarity\n",
        "# C is the inverse of regularization strength (smaller C = stronger regularization)\n",
        "model_l2 = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1.0,  # You can adjust this for different regularization strengths\n",
        "    solver='lbfgs',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model_l2.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Print model coefficients\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"L2 REGULARIZED LOGISTIC REGRESSION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nModel Coefficients (for each class):\")\n",
        "for i, class_name in enumerate(wine.target_names):\n",
        "    print(f\"\\nClass {i} ({class_name}):\")\n",
        "    coefficients = model_l2.coef_[i]\n",
        "    for j, feature_name in enumerate(wine.feature_names):\n",
        "        print(f\"  {feature_name}: {coefficients[j]:.4f}\")\n",
        "\n",
        "print(\"\\nIntercepts for each class:\")\n",
        "for i, class_name in enumerate(wine.target_names):\n",
        "    print(f\"  Class {i} ({class_name}): {model_l2.intercept_[i]:.4f}\")\n",
        "\n",
        "# Step 6: Make predictions and calculate accuracy\n",
        "y_pred = model_l2.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(f\"Model Accuracy on Test Set: {accuracy:.4f}\")\n",
        "print(f\"Accuracy Percentage: {accuracy * 100:.2f}%\")\n",
        "print(f\"Number of correct predictions: {np.sum(y_pred == y_test)}/{len(y_test)}\")\n",
        "\n",
        "# Additional: Show the effect of different regularization strengths\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EFFECT OF DIFFERENT REGULARIZATION STRENGTHS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "C_values = [0.01, 0.1, 1, 10, 100]\n",
        "for C in C_values:\n",
        "    model = LogisticRegression(penalty='l2', C=C, solver='lbfgs', max_iter=1000, random_state=42)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"C = {C:6.2f} (regularization = {1/C:6.2f}): Accuracy = {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGZ6F4jyqyT_",
        "outputId": "c56b886c-2ac1-44d8-ca17-5eda215a6155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wine Dataset Information:\n",
            "Number of samples: 178\n",
            "Number of features: 13\n",
            "Number of classes: 3\n",
            "Feature names: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
            "Target names: ['class_0' 'class_1' 'class_2']\n",
            "\n",
            "============================================================\n",
            "L2 REGULARIZED LOGISTIC REGRESSION RESULTS\n",
            "============================================================\n",
            "\n",
            "Model Coefficients (for each class):\n",
            "\n",
            "Class 0 (class_0):\n",
            "  alcohol: 0.7469\n",
            "  malic_acid: 0.0919\n",
            "  ash: 0.3996\n",
            "  alcalinity_of_ash: -0.8327\n",
            "  magnesium: 0.1115\n",
            "  total_phenols: 0.3233\n",
            "  flavanoids: 0.6736\n",
            "  nonflavanoid_phenols: 0.0039\n",
            "  proanthocyanins: -0.0021\n",
            "  color_intensity: 0.0905\n",
            "  hue: 0.0606\n",
            "  od280/od315_of_diluted_wines: 0.5664\n",
            "  proline: 0.8697\n",
            "\n",
            "Class 1 (class_1):\n",
            "  alcohol: -0.9548\n",
            "  malic_acid: -0.3809\n",
            "  ash: -0.7751\n",
            "  alcalinity_of_ash: 0.5929\n",
            "  magnesium: -0.1889\n",
            "  total_phenols: -0.1210\n",
            "  flavanoids: 0.1964\n",
            "  nonflavanoid_phenols: -0.0053\n",
            "  proanthocyanins: 0.5452\n",
            "  color_intensity: -0.8048\n",
            "  hue: 0.7054\n",
            "  od280/od315_of_diluted_wines: -0.0069\n",
            "  proline: -0.9394\n",
            "\n",
            "Class 2 (class_2):\n",
            "  alcohol: 0.2080\n",
            "  malic_acid: 0.2890\n",
            "  ash: 0.3755\n",
            "  alcalinity_of_ash: 0.2398\n",
            "  magnesium: 0.0774\n",
            "  total_phenols: -0.2023\n",
            "  flavanoids: -0.8700\n",
            "  nonflavanoid_phenols: 0.0014\n",
            "  proanthocyanins: -0.5431\n",
            "  color_intensity: 0.7144\n",
            "  hue: -0.7660\n",
            "  od280/od315_of_diluted_wines: -0.5595\n",
            "  proline: 0.0697\n",
            "\n",
            "Intercepts for each class:\n",
            "  Class 0 (class_0): 0.3433\n",
            "  Class 1 (class_1): 0.8270\n",
            "  Class 2 (class_2): -1.1703\n",
            "\n",
            "------------------------------------------------------------\n",
            "Model Accuracy on Test Set: 0.9815\n",
            "Accuracy Percentage: 98.15%\n",
            "Number of correct predictions: 53/54\n",
            "\n",
            "============================================================\n",
            "EFFECT OF DIFFERENT REGULARIZATION STRENGTHS\n",
            "============================================================\n",
            "C =   0.01 (regularization = 100.00): Accuracy = 1.0000\n",
            "C =   0.10 (regularization =  10.00): Accuracy = 1.0000\n",
            "C =   1.00 (regularization =   1.00): Accuracy = 0.9815\n",
            "C =  10.00 (regularization =   0.10): Accuracy = 0.9815\n",
            "C = 100.00 (regularization =   0.01): Accuracy = 0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 7: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report.  (Use Dataset from sklearn package )\n"
      ],
      "metadata": {
        "id": "tw3MSHZw2ZNT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYH6cOIXo1ee",
        "outputId": "9c04278f-07c1-4d8f-ee9b-41f9b364a992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wine Dataset - Classification Report (OvR):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0       1.00      1.00      1.00        19\n",
            "     class_1       1.00      0.95      0.98        21\n",
            "     class_2       0.93      1.00      0.97        14\n",
            "\n",
            "    accuracy                           0.98        54\n",
            "   macro avg       0.98      0.98      0.98        54\n",
            "weighted avg       0.98      0.98      0.98        54\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import required Libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train OvR Logistic Regression\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Classification Report\n",
        "print(\"Wine Dataset - Classification Report (OvR):\")\n",
        "print(classification_report(y_test, y_pred, target_names=wine.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 8: Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy. (Use Dataset from sklearn package)\n"
      ],
      "metadata": {
        "id": "hzIoNkZz6y3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "print(\"BREAST CANCER DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Classes: {data.target_names}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Step 2: Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")\n",
        "\n",
        "# Step 3: Scale the features (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],  # Regularization type\n",
        "    'solver': ['liblinear']  # Solver that supports both L1 and L2\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HYPERPARAMETER GRID\")\n",
        "print(\"=\"*60)\n",
        "print(f\"C values: {param_grid['C']}\")\n",
        "print(f\"Penalty types: {param_grid['penalty']}\")\n",
        "print(f\"Total combinations: {len(param_grid['C']) * len(param_grid['penalty'])}\")\n",
        "\n",
        "# Step 5: Create Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Step 6: Create GridSearchCV object\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,  # Use all available processors\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 7: Fit GridSearchCV\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMING GRID SEARCH...\")\n",
        "print(\"=\"*60)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 8: Print the best parameters and validation accuracy\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GRID SEARCH RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Step 9: Get the best model and evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nTest Set Accuracy with Best Model: {test_accuracy:.4f}\")\n",
        "\n",
        "# Step 10: Show all results from grid search\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL GRID SEARCH RESULTS (sorted by mean CV score)\")\n",
        "print(\"=\"*60)\n",
        "results = grid_search.cv_results_\n",
        "for i in range(len(results['params'])):\n",
        "    print(f\"Rank {results['rank_test_score'][i]}: \"\n",
        "          f\"C={results['params'][i]['C']}, \"\n",
        "          f\"penalty={results['params'][i]['penalty']} - \"\n",
        "          f\"CV Accuracy: {results['mean_test_score'][i]:.4f} \"\n",
        "          f\"(+/- {results['std_test_score'][i]*2:.4f})\")\n",
        "\n",
        "# Step 11: Classification report with best model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASSIFICATION REPORT (Best Model)\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSUXuNLU6yZN",
        "outputId": "cb55cd29-dec6-4a29-8add-887fe66a03f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BREAST CANCER DATASET\n",
            "============================================================\n",
            "Number of samples: 569\n",
            "Number of features: 30\n",
            "Classes: ['malignant' 'benign']\n",
            "Class distribution: [212 357]\n",
            "\n",
            "Training samples: 398\n",
            "Testing samples: 171\n",
            "\n",
            "============================================================\n",
            "HYPERPARAMETER GRID\n",
            "============================================================\n",
            "C values: [0.001, 0.01, 0.1, 1, 10, 100]\n",
            "Penalty types: ['l1', 'l2']\n",
            "Total combinations: 12\n",
            "\n",
            "============================================================\n",
            "PERFORMING GRID SEARCH...\n",
            "============================================================\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "\n",
            "============================================================\n",
            "GRID SEARCH RESULTS\n",
            "============================================================\n",
            "Best Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Best Cross-Validation Accuracy: 0.9799\n",
            "\n",
            "Test Set Accuracy with Best Model: 0.9883\n",
            "\n",
            "============================================================\n",
            "ALL GRID SEARCH RESULTS (sorted by mean CV score)\n",
            "============================================================\n",
            "Rank 12: C=0.001, penalty=l1 - CV Accuracy: 0.3718 (+/- 0.0078)\n",
            "Rank 10: C=0.001, penalty=l2 - CV Accuracy: 0.9422 (+/- 0.0562)\n",
            "Rank 11: C=0.01, penalty=l1 - CV Accuracy: 0.9146 (+/- 0.0796)\n",
            "Rank 5: C=0.01, penalty=l2 - CV Accuracy: 0.9724 (+/- 0.0292)\n",
            "Rank 4: C=0.1, penalty=l1 - CV Accuracy: 0.9749 (+/- 0.0386)\n",
            "Rank 2: C=0.1, penalty=l2 - CV Accuracy: 0.9799 (+/- 0.0123)\n",
            "Rank 3: C=1, penalty=l1 - CV Accuracy: 0.9774 (+/- 0.0291)\n",
            "Rank 1: C=1, penalty=l2 - CV Accuracy: 0.9799 (+/- 0.0299)\n",
            "Rank 8: C=10, penalty=l1 - CV Accuracy: 0.9548 (+/- 0.0490)\n",
            "Rank 6: C=10, penalty=l2 - CV Accuracy: 0.9623 (+/- 0.0448)\n",
            "Rank 9: C=100, penalty=l1 - CV Accuracy: 0.9523 (+/- 0.0556)\n",
            "Rank 7: C=100, penalty=l2 - CV Accuracy: 0.9598 (+/- 0.0510)\n",
            "\n",
            "============================================================\n",
            "CLASSIFICATION REPORT (Best Model)\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.98      0.98      0.98        64\n",
            "      benign       0.99      0.99      0.99       107\n",
            "\n",
            "    accuracy                           0.99       171\n",
            "   macro avg       0.99      0.99      0.99       171\n",
            "weighted avg       0.99      0.99      0.99       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9: Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.  (Use Dataset from sklearn package)\n"
      ],
      "metadata": {
        "id": "_xayoDHM8pDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Digits dataset from sklearn\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "print(\"DATASET: Handwritten Digits Recognition\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Number of classes: {len(np.unique(y))}\")\n",
        "\n",
        "# Show feature value ranges (demonstrates need for scaling)\n",
        "print(f\"\\nFeature value range: [{X.min():.2f}, {X.max():.2f}]\")\n",
        "print(f\"Mean feature value: {X.mean():.2f}\")\n",
        "print(f\"Std deviation: {X.std():.2f}\")\n",
        "\n",
        "# Step 2: Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "\n",
        "# Step 3: Train Logistic Regression WITHOUT Scaling\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL 1: LOGISTIC REGRESSION WITHOUT SCALING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create and train model without scaling\n",
        "model_no_scale = LogisticRegression(max_iter=5000, random_state=42)\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and accuracy\n",
        "y_pred_no_scale = model_no_scale.predict(X_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "print(f\"Training completed!\")\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scale:.4f}\")\n",
        "\n",
        "# Step 4: Train Logistic Regression WITH Scaling\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL 2: LOGISTIC REGRESSION WITH STANDARDIZATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Verify scaling worked\n",
        "print(f\"After scaling - Mean: {X_train_scaled.mean():.2e}\")\n",
        "print(f\"After scaling - Std: {X_train_scaled.std():.2f}\")\n",
        "\n",
        "# Create and train model with scaling\n",
        "model_with_scale = LogisticRegression(max_iter=5000, random_state=42)\n",
        "model_with_scale.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions and accuracy\n",
        "y_pred_with_scale = model_with_scale.predict(X_test_scaled)\n",
        "accuracy_with_scale = accuracy_score(y_test, y_pred_with_scale)\n",
        "\n",
        "print(f\"Training completed!\")\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scale:.4f}\")\n",
        "\n",
        "# Step 5: Compare Results\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPARISON OF RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy WITHOUT Scaling: {accuracy_no_scale:.4f} ({accuracy_no_scale*100:.2f}%)\")\n",
        "print(f\"Accuracy WITH Scaling:    {accuracy_with_scale:.4f} ({accuracy_with_scale*100:.2f}%)\")\n",
        "print(f\"\\nDifference: {abs(accuracy_with_scale - accuracy_no_scale):.4f}\")\n",
        "\n",
        "if accuracy_with_scale > accuracy_no_scale:\n",
        "    improvement = ((accuracy_with_scale - accuracy_no_scale) / accuracy_no_scale) * 100\n",
        "    print(f\"Scaling improved accuracy by {improvement:.2f}%\")\n",
        "elif accuracy_no_scale > accuracy_with_scale:\n",
        "    decrease = ((accuracy_no_scale - accuracy_with_scale) / accuracy_no_scale) * 100\n",
        "    print(f\"Scaling decreased accuracy by {decrease:.2f}%\")\n",
        "else:\n",
        "    print(\"Both models achieved the same accuracy\")\n",
        "\n",
        "# Additional Analysis\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ADDITIONAL INSIGHTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check convergence\n",
        "print(f\"Iterations to converge (no scaling): {model_no_scale.n_iter_[0]}\")\n",
        "print(f\"Iterations to converge (with scaling): {model_with_scale.n_iter_[0]}\")\n",
        "\n",
        "# Number of misclassified samples\n",
        "misclassified_no_scale = np.sum(y_test != y_pred_no_scale)\n",
        "misclassified_with_scale = np.sum(y_test != y_pred_with_scale)\n",
        "\n",
        "print(f\"\\nMisclassified samples (no scaling): {misclassified_no_scale}/{len(y_test)}\")\n",
        "print(f\"Misclassified samples (with scaling): {misclassified_with_scale}/{len(y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA3re-tM8rh7",
        "outputId": "9b0c661a-364c-4844-f299-897451780bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATASET: Handwritten Digits Recognition\n",
            "==================================================\n",
            "Number of samples: 1797\n",
            "Number of features: 64\n",
            "Number of classes: 10\n",
            "\n",
            "Feature value range: [0.00, 16.00]\n",
            "Mean feature value: 4.88\n",
            "Std deviation: 6.02\n",
            "\n",
            "Train set size: 1257\n",
            "Test set size: 540\n",
            "\n",
            "==================================================\n",
            "MODEL 1: LOGISTIC REGRESSION WITHOUT SCALING\n",
            "==================================================\n",
            "Training completed!\n",
            "Accuracy without scaling: 0.9685\n",
            "\n",
            "==================================================\n",
            "MODEL 2: LOGISTIC REGRESSION WITH STANDARDIZATION\n",
            "==================================================\n",
            "After scaling - Mean: -2.83e-18\n",
            "After scaling - Std: 0.98\n",
            "Training completed!\n",
            "Accuracy with scaling: 0.9704\n",
            "\n",
            "==================================================\n",
            "COMPARISON OF RESULTS\n",
            "==================================================\n",
            "Accuracy WITHOUT Scaling: 0.9685 (96.85%)\n",
            "Accuracy WITH Scaling:    0.9704 (97.04%)\n",
            "\n",
            "Difference: 0.0019\n",
            "Scaling improved accuracy by 0.19%\n",
            "\n",
            "==================================================\n",
            "ADDITIONAL INSIGHTS\n",
            "==================================================\n",
            "Iterations to converge (no scaling): 140\n",
            "Iterations to converge (with scaling): 32\n",
            "\n",
            "Misclassified samples (no scaling): 17/540\n",
            "Misclassified samples (with scaling): 16/540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 10: Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case."
      ],
      "metadata": {
        "id": "UFxLPmNz-GiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When I have an imbalanced dataset — only 5% of customers respond (the positive class) — it's crucial that I handle data and model training carefully so that the model doesn't simply predict \"no response\" for everyone. My practical approach would involve:\n",
        "\n",
        "1.\tData Collection and Cleaning:\n",
        "I would gather relevant customer features (e.g., demographics, purchase history, browsing behavior, etc.). I would clean the data by handling missing values, outliers, and inconsistent entries. I would convert categorical variables to appropriate encodings (e.g., One-Hot Encoding).\n",
        "\n",
        "2.\tBalancing the Classes:\n",
        "Since only 5% respond, the dataset is heavily imbalanced. Without addressing this, the model might predict \"no response\" for nearly everyone and have high accuracy but poor recall for responders. I would use techniques such as:\n",
        "\n",
        "3.\tOversampling the minority class (e.g., SMOTE)\n",
        "Undersampling the majority class\n",
        "Adjusting class weights in Logistic Regression (parameter \"class_weight='balanced'\")\n",
        "Feature Scaling:\n",
        "I would standardize or normalize numerical features using StandardScaler or MinMaxScaler to ensure that features with large numerical ranges do not dominate the model coefficients.\n",
        "\n",
        "4.\tModel Training with Logistic Regression:\n",
        "I would start with a baseline logistic regression. I would incorporate regularization (L1 or L2) to handle overfitting, particularly if the feature space is large. If using scikit-learn, I would consider setting class_weight='balanced' to make the algorithm pay more attention to the minority class.\n",
        "\n",
        "5.\tHyperparameter Tuning:\n",
        "I would use GridSearchCV or RandomizedSearchCV to find the best regularization parameter (C) and penalty type (L1 or L2). I would possibly tune solver and class_weight to optimize performance.\n",
        "\n",
        "6.\tEvaluation Metrics:\n",
        "Accuracy alone can be misleading with highly imbalanced data. Instead, I would focus on:\n",
        "\n",
        "- Precision: Of those predicted to respond, how many actually responded?\n",
        "- Recall (or Sensitivity): Of all actual responders, how many did we correctly identify?\n",
        "- F1-Score: Balances precision and recall\n",
        "- ROC-AUC: Measures overall separability across different thresholds\n",
        "- PR (Precision-Recall) AUC: Particularly useful in heavily imbalanced scenarios\n",
        "\n",
        "I would consider business requirements. For example, a higher Recall might be desired if I want to target as many potential responders as possible. However, that might also increase marketing costs if the precision is too low.\n",
        "\n",
        "**Iterative Improvement and Real-World Constraints:**\n",
        "I would regularly update and retrain the model with new data to capture changing customer behaviors. It might be beneficial for me to create segmentation strategies, focusing on certain groups for more personalized campaigns. I would monitor the model's performance in production, track the real response rates, and adjust thresholds or re-balance as needed.\n",
        "\n",
        "By following these steps — focusing on class balancing, proper evaluation metrics, and iterative tuning — my Logistic Regression model can better capture the minority class and provide valuable predictions for a marketing campaign."
      ],
      "metadata": {
        "id": "GTSl_Mtx-LDv"
      }
    }
  ]
}