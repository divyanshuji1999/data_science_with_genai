{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**ENSEMBLE ASSIGNEMT**"
      ],
      "metadata": {
        "id": "LqEk7iGGi5C3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "Answer :  \n",
        "Ensemble Learning is a machine learning technique where I combine multiple models (also called base learners or weak learners) to make a stronger and more accurate model. The central idea is that a group of models working together can perform better than any single model.  \n",
        "\n",
        "- It helps to reduce variance, bias, and improve overall prediction accuracy.  \n",
        "- Different models capture different patterns in the data, so when combined, their strengths are added and weaknesses are minimized.  \n",
        "- Common techniques include Bagging, Boosting, and Stacking.  \n",
        "- Real-world example: Random Forest (an ensemble of many decision trees).  \n"
      ],
      "metadata": {
        "id": "HyQ-5OJBcuB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer :  \n",
        "Bagging and Boosting are both ensemble learning methods but they work in different ways:  \n",
        "\n",
        "- **Bagging (Bootstrap Aggregating):**  \n",
        "  - Multiple models are trained in parallel on different bootstrap samples (sampling with replacement).  \n",
        "  - The final prediction is made by combining the results (majority vote for classification or average for regression).  \n",
        "  - Its main goal is to reduce variance and avoid overfitting.  \n",
        "  - Example: Random Forest.  \n",
        "\n",
        "- **Boosting:**  \n",
        "  - Models are trained sequentially, where each new model focuses on correcting the errors made by the previous ones.  \n",
        "  - The models are combined with weights, giving more importance to stronger learners.  \n",
        "  - Its main goal is to reduce both bias and variance.  \n",
        "  - Example: AdaBoost, Gradient Boosting, XGBoost.  \n",
        "\n",
        "In short, Bagging builds models independently in parallel to reduce variance, while Boosting builds models sequentially to reduce bias and improve accuracy.\n"
      ],
      "metadata": {
        "id": "TsW3Hg0FeLRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Answer :  \n",
        "Bootstrap sampling is a statistical technique where I create new datasets by randomly selecting samples **with replacement** from the original training data. Each bootstrap sample is usually the same size as the original dataset, but because of replacement, some records appear multiple times while some may not appear at all.  \n",
        "\n",
        "In Bagging methods like Random Forest:  \n",
        "- Each decision tree is trained on a different bootstrap sample, which ensures diversity among the trees.  \n",
        "- This randomness reduces correlation between the trees and makes the overall model more stable.  \n",
        "- When predictions of all trees are combined (through voting or averaging), the final result becomes more accurate and less prone to overfitting compared to a single decision tree.  \n"
      ],
      "metadata": {
        "id": "0dL7UKjSetoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Answer :  \n",
        "In bootstrap sampling, since data is selected with replacement, almost one-third of the data is left out in each bootstrap sample. These left-out records are called **Out-of-Bag (OOB) samples**.  \n",
        "\n",
        "In ensemble models like Random Forest:  \n",
        "- Each tree is trained on its bootstrap sample, and the corresponding OOB samples for that tree can be used as test data.  \n",
        "- This allows me to estimate the modelâ€™s performance without needing a separate validation set.  \n",
        "- The **OOB score** is simply the accuracy (or error) calculated using these OOB predictions.  \n",
        "- It is very useful because it provides an unbiased estimate of model performance and helps save data for training instead of splitting into validation sets.  \n"
      ],
      "metadata": {
        "id": "RdKabqQme5Vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Answer :  \n",
        "In a **single Decision Tree**:  \n",
        "- Feature importance is calculated based on how much a feature reduces impurity (like Gini Index or Entropy) across all its splits.  \n",
        "- However, the importance can be biased if one tree happens to favor certain features due to randomness or noise in the data.  \n",
        "\n",
        "In a **Random Forest**:  \n",
        "- Importance is averaged across many trees trained on different bootstrap samples and random feature subsets.  \n",
        "- This reduces bias and gives a more reliable and stable measure of which features are truly important.  \n",
        "- Hence, feature importance from Random Forest is generally considered more trustworthy than that from a single Decision Tree.  \n"
      ],
      "metadata": {
        "id": "VUN0UP1ge8x5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. Write a Python program to:\n",
        "- Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "- Train a Random Forest Classifier\n",
        "- Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "4fra2oIygbi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "top5 = importances.sort_values(ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 important features:\\n\")\n",
        "print(top5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKUkQoXagcRR",
        "outputId": "055b1838-659b-4102-e4c4-4e85c4888676"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 important features:\n",
            "\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7. Write a Python program to:\n",
        "- Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "- Evaluate its accuracy and compare with a single Decision Tree\n"
      ],
      "metadata": {
        "id": "y1IE-ml-gldx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer :\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "bag = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "y_pred_bag = bag.predict(X_test)\n",
        "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "print(\"Accuracy of Single Decision Tree :\", acc_dt)\n",
        "print(\"Accuracy of Bagging Classifier   :\", acc_bag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlLn4kKpggjN",
        "outputId": "cb124295-2cc6-4869-ba94-624f2522a6af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree : 0.9333333333333333\n",
            "Accuracy of Bagging Classifier   : 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8. Write a Python program to:\n",
        "- Train a Random Forest Classifier\n",
        "- Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "- Print the best parameters and final accuracy\n"
      ],
      "metadata": {
        "id": "o3RFGUYlhJod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer :\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 150],\n",
        "    \"max_depth\": [None, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid.best_params_\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = grid.best_estimator_.predict(X_test)\n",
        "final_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters :\", best_params)\n",
        "print(\"Final Accuracy  :\", final_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk3fd86_gv-Y",
        "outputId": "9f2fc062-5dde-44f6-f81b-635a7e00f269"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters : {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy  : 0.935672514619883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9. Write a Python program to:\n",
        "- Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "- Compare their Mean Squared Errors (MSE)\n"
      ],
      "metadata": {
        "id": "zDzgtY7xhUps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer :\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "data = fetch_california_housing()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "\n",
        "bag = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "y_pred_bag = bag.predict(X_test)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Mean Squared Error of Bagging Regressor      :\", mse_bag)\n",
        "print(\"Mean Squared Error of Random Forest Regressor:\", mse_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zv0hGiYhLtx",
        "outputId": "778957fb-ffaa-4e7f-b952-7d088bc59325"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of Bagging Regressor      : 0.25787382250585034\n",
            "Mean Squared Error of Random Forest Regressor: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to:\n",
        "- Choose between Bagging or Boosting\n",
        "- Handle overfitting\n",
        "- Select base models\n",
        "- Evaluate performance using cross-validation\n",
        "- Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "Answer :  \n",
        "If I have to build a loan default prediction model, my approach will be as follows:\n",
        "\n",
        "- **Choice between Bagging and Boosting**: I will first try Boosting (like XGBoost or AdaBoost) because boosting generally works better when data has complex patterns and imbalance, which is common in loan default cases. Bagging (like Random Forest) can also be tested for stability, but Boosting usually gives higher accuracy.\n",
        "- **Handling Overfitting**: I will use techniques like limiting the max_depth of trees, adding regularization (learning rate, min_samples_split), and using early stopping while training Boosting models to avoid overfitting.\n",
        "- **Selecting Base Models**: Decision Trees will be my base learners since they are flexible and work well as weak learners in boosting and bagging frameworks.\n",
        "- **Evaluating with Cross-Validation**: I will apply k-fold cross-validation to make sure my model performance is stable and not just by chance on one split of data. I will track metrics like accuracy, precision, recall, and ROC-AUC since in loan default prediction, false negatives are very costly.\n",
        "- **Justification**: Using ensemble learning reduces variance (in Bagging) or bias (in Boosting). This makes the predictions more robust and reliable, which helps the financial institution reduce risk, improve loan approval decisions, and minimize financial losses.\n",
        "\n"
      ],
      "metadata": {
        "id": "w-7iaNcWhbr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer :\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# For demonstration, let's use sklearn's make_classification to simulate loan default dataset\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(\n",
        "    n_samples=5000, n_features=20, n_informative=10, n_redundant=5,\n",
        "    n_classes=2, weights=[0.7, 0.3], random_state=42\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "\n",
        "# GridSearchCV to tune hyperparameters\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100],\n",
        "    \"max_depth\": [3, 5],\n",
        "    \"learning_rate\": [0.05, 0.1]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=GradientBoostingClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"roc_auc\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters :\", grid.best_params_)\n",
        "print(\"Accuracy on Test Set :\", accuracy_score(y_test, y_pred))\n",
        "print(\"ROC-AUC Score      :\", roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1]))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhNYj28BhWtu",
        "outputId": "a75a2feb-2327-4022-8a99-a529d97c2d1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters : {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
            "Accuracy on Test Set : 0.9373333333333334\n",
            "ROC-AUC Score      : 0.9687976368938057\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96      1047\n",
            "           1       0.94      0.85      0.89       453\n",
            "\n",
            "    accuracy                           0.94      1500\n",
            "   macro avg       0.94      0.91      0.92      1500\n",
            "weighted avg       0.94      0.94      0.94      1500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HnoUdlOUhoFg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}