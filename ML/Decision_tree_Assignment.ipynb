{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**DECISION TREE ASSIGNMENT**"
      ],
      "metadata": {
        "id": "jQ0styjchcHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "A **Decision Tree** is a supervised machine learning model that is structured like a flowchart.  \n",
        "- **Nodes** represent conditions or questions on features.  \n",
        "- **Branches** represent the outcomes of those conditions.  \n",
        "- **Leaf nodes** represent the final prediction or class label.  \n",
        "\n",
        "In the context of **classification**:  \n",
        "- The dataset is split step by step based on the feature values that best separate the classes.  \n",
        "- At each node, the tree asks a simple question (e.g., *Is age > 30?*).  \n",
        "- Depending on the answer, the data follows the corresponding branch.  \n",
        "- This continues until a leaf node is reached, which gives the predicted class.  \n",
        "\n",
        "Decision trees are popular because they are easy to interpret and closely resemble human decision-making.  \n",
        "\n"
      ],
      "metadata": {
        "id": "4jitrOd6hLVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Question 2 : Explain the concepts of Gini Impurity and Entropy as impurity measures.  \n",
        "### How do they impact the splits in a Decision Tree?\n",
        "\n",
        "**Gini Impurity** and **Entropy** are two popular measures used to check how \"pure\" or \"impure\" a node is in a decision tree.\n",
        "\n",
        "- **Gini Impurity**  \n",
        "  - Measures the probability that a randomly chosen sample would be misclassified.  \n",
        "  - Formula: `Gini = 1 - Σ(pᵢ²)`  \n",
        "  - Value is **0** when all samples belong to one class (pure), and higher when classes are mixed.  \n",
        "\n",
        "- **Entropy**  \n",
        "  - Measures the amount of uncertainty or disorder in the data.  \n",
        "  - Formula: `Entropy = - Σ(pᵢ * log₂(pᵢ))`  \n",
        "  - Value is **0** for a pure node, and higher for mixed distributions.  \n",
        "\n",
        "**Impact on splits:**  \n",
        "- Both Gini and Entropy guide the tree to choose the **best split**.  \n",
        "- The algorithm calculates impurity for each possible split and selects the one that reduces impurity the most (highest Information Gain for Entropy or lowest Gini).  \n",
        "- This ensures that the tree keeps dividing data into groups that are as pure as possible.  \n",
        "\n",
        "In short, these measures help the tree grow in a way that improves classification accuracy.  \n"
      ],
      "metadata": {
        "id": "IKQpfmzNh-kO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees?  \n",
        "### Give one practical advantage of using each.\n",
        "\n",
        "**Pre-Pruning (Early Stopping):**  \n",
        "- In pre-pruning, the growth of the tree is restricted before it becomes too complex.  \n",
        "- This can be done by setting limits such as maximum depth, minimum samples per leaf, or minimum information gain.  \n",
        "- By stopping early, the tree avoids creating branches that do not add much value.  \n",
        "- **Practical Advantage:** It reduces training time and computational cost, which is very helpful when dealing with large datasets.  \n",
        "- **Example:** In a loan approval system, if splitting further on a feature like “favorite color” does not improve accuracy, the tree will stop there, saving time and avoiding useless rules.  \n",
        "\n",
        "**Post-Pruning (Pruning after Full Growth):**  \n",
        "- In post-pruning, the tree is allowed to grow fully first and then unnecessary branches are removed.  \n",
        "- This is usually done using a validation set to check which branches hurt performance.  \n",
        "- The final pruned tree becomes simpler and more general.  \n",
        "- **Practical Advantage:** It improves prediction accuracy on unseen data by reducing overfitting, making the model more reliable in real-world scenarios.  \n",
        "- **Example:** In a medical diagnosis model, a fully grown tree might create a very specific rule like *“If age = 37 and blood pressure = 122, then Disease X”*. Post-pruning can remove such overfitted branches, keeping only the general patterns that work for most patients.  \n",
        "\n",
        "Overall, pre-pruning saves time and resources during training, while post-pruning improves generalization by removing unnecessary complexity.  \n"
      ],
      "metadata": {
        "id": "7n9yG0Arh-gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "**Information Gain (IG):**  \n",
        "- Information Gain is a measure that tells us how much \"uncertainty\" or impurity is reduced after splitting the dataset on a feature.  \n",
        "- It is calculated as the difference between the impurity of the parent node and the weighted impurity of the child nodes.  \n",
        "- Formula:  \n",
        "  `IG = Impurity(parent) – [weighted average of Impurity(children)]`  \n",
        "\n",
        "**Why it is important:**  \n",
        "- At each step, a decision tree has to decide **which feature to split on**.  \n",
        "- Information Gain helps select the feature that creates the **purest child nodes**, meaning the data points inside each node are more similar (belong to the same class).  \n",
        "- The higher the Information Gain, the better the feature is for splitting.  \n",
        "\n",
        "**Example:**  \n",
        "Suppose we have 10 samples:  \n",
        "- 6 belong to Class A, and 4 belong to Class B.  \n",
        "- Parent Entropy = `-[(6/10) log₂(6/10) + (4/10) log₂(4/10)] ≈ 0.97`  \n",
        "\n",
        "Now, if we split based on a feature:  \n",
        "- Left child: 4 samples (all Class A) → Entropy = 0 (pure).  \n",
        "- Right child: 6 samples (2 Class A, 4 Class B) → Entropy ≈ 0.92.  \n",
        "- Weighted Child Entropy = `(4/10 * 0) + (6/10 * 0.92) ≈ 0.55`  \n",
        "\n",
        "So,  \n",
        "`IG = 0.97 – 0.55 = 0.42`  \n",
        "\n",
        "This shows that the split reduces impurity by **0.42**, meaning it’s a good choice for the tree.\n"
      ],
      "metadata": {
        "id": "IiBHWBH6h-d8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "**Real-world applications:**  \n",
        "- **Medical Diagnosis:** Used to predict diseases based on symptoms and patient history.  \n",
        "- **Finance:** Helps in credit scoring and loan approval decisions.  \n",
        "- **Marketing:** Used for customer segmentation and predicting buying behavior.  \n",
        "- **Fraud Detection:** Identifies unusual patterns in transactions.  \n",
        "- **Manufacturing:** Assists in quality control and defect detection.  \n",
        "\n",
        "**Main Advantages:**  \n",
        "- Easy to understand and interpret, even for non-technical users.  \n",
        "- Handles both numerical and categorical data.  \n",
        "- Requires little data preprocessing compared to other algorithms.  \n",
        "- Works well for small to medium datasets.  \n",
        "\n",
        "**Main Limitations:**  \n",
        "- Prone to overfitting, especially with deep trees.  \n",
        "- Can be unstable, as small changes in data may lead to a completely different tree.  \n",
        "- Greedy splitting may not always result in the most optimal tree.  \n",
        "- Less effective on very large datasets compared to ensemble methods like Random Forests.  \n"
      ],
      "metadata": {
        "id": "qzG2OCEOh-bL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 6: Write a Python program to:\n",
        "- Load the Iris Dataset  \n",
        "- Train a Decision Tree Classifier using the Gini criterion  \n",
        "- Print the model’s accuracy and feature importances  \n"
      ],
      "metadata": {
        "id": "dwgR8KVAh-YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier with Gini criterion\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Model accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeIUIeqQpLpY",
        "outputId": "db0cebae-3ca7-4bc4-8e0c-ff8db5e3cd45"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 7: Write a Python program to:\n",
        "- Load the Iris Dataset  \n",
        "- Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree  \n"
      ],
      "metadata": {
        "id": "YERlJ-yAh-Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dataset\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and accuracy for depth=3\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "# Train a fully-grown Decision Tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and accuracy for full tree\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy with max_depth=3:\", accuracy_depth3)\n",
        "print(\"Accuracy with fully-grown tree:\", accuracy_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK3WkD8kqMW9",
        "outputId": "1769540b-8790-4563-b2de-1ed9beff2fd3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0\n",
            "Accuracy with fully-grown tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 8: Write a Python program to:\n",
        "- Load the Boston Housing Dataset  \n",
        "- Train a Decision Tree Regressor  \n",
        "- Print the Mean Squared Error (MSE) and feature importances  \n"
      ],
      "metadata": {
        "id": "TJnGYQ6dh-S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load Boston Housing dataset from OpenML\n",
        "from sklearn.datasets import fetch_openml\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "\n",
        "# Features and target\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Mean Squared Error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# Feature Importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(X.columns, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dGVus-ArXaM",
        "outputId": "4ce03e65-e3ab-440c-f11b-75693a75ea25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 11.588026315789474\n",
            "\n",
            "Feature Importances:\n",
            "CRIM: 0.0585\n",
            "ZN: 0.0010\n",
            "INDUS: 0.0099\n",
            "CHAS: 0.0003\n",
            "NOX: 0.0071\n",
            "RM: 0.5758\n",
            "AGE: 0.0072\n",
            "DIS: 0.1096\n",
            "RAD: 0.0016\n",
            "TAX: 0.0022\n",
            "PTRATIO: 0.0250\n",
            "B: 0.0119\n",
            "LSTAT: 0.1900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 9: Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "- Print the best parameters and the resulting model accuracy\n"
      ],
      "metadata": {
        "id": "Rw9-tryKh-P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load Iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Decision Tree Classifier + GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 4, 6, 8, 10]\n",
        "}\n",
        "\n",
        "# Grid search with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Model Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSDy-ggesNR2",
        "outputId": "16dbfdbf-8cc3-4864-956a-78396ae2ed05"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 6}\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:**\n",
        "- Handle the missing values\n",
        "- Encode the categorical features\n",
        "- Train a Decision Tree model\n",
        "- Tune its hyperparameters\n",
        "- Evaluate its performance And describe what business value this model could provide in the real-world\n",
        "setting.\n"
      ],
      "metadata": {
        "id": "3jsqic7Nh968"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer : If I am working as a data scientist for a healthcare company, I would approach this problem step by step :\n",
        "\n",
        "**Step 1: Handle Missing Values**\n",
        "\n",
        "- First, I’d explore the dataset to see where and how much data is missing.\n",
        "\n",
        "- For numerical features, I could use mean/median imputation, or if the missingness is not random, try more advanced methods like KNN imputation.\n",
        "\n",
        "- For categorical features, I’d use the mode (most frequent category) or a separate category like \"Unknown\".\n",
        "\n",
        "- If a feature has too many missing values (say >50%), I’d consider dropping it after checking its importance.\n",
        "\n",
        "**Step 2: Encode Categorical Features**\n",
        "\n",
        "- Decision Trees don’t need feature scaling, but they do require numbers.\n",
        "\n",
        "- I’d use One-Hot Encoding for nominal categories (like blood type) and Ordinal Encoding if categories have an order (like severity levels: mild → moderate → severe).\n",
        "\n",
        "- Libraries like pandas.get_dummies() or sklearn.preprocessing.OneHotEncoder would be handy here.\n",
        "\n",
        "**Step 3: Train a Decision Tree Model**\n",
        "\n",
        "- Split the dataset into train and test sets (e.g., 70:30).\n",
        "\n",
        "- Train a DecisionTreeClassifier from scikit-learn with basic parameters first.\n",
        "\n",
        "- Since trees can easily overfit, I’d keep an eye on tree depth, min_samples_split, etc.\n",
        "\n",
        "**Step 4: Tune Hyperparameters**\n",
        "\n",
        "- Use GridSearchCV or RandomizedSearchCV to find the best combination of:\n",
        "\n",
        " - max_depth (to prevent overfitting)\n",
        "\n",
        " - min_samples_split and min_samples_leaf (to control branching)\n",
        "\n",
        " - criterion (Gini vs Entropy)\n",
        "\n",
        "- Perform cross-validation to make sure results are stable across folds.\n",
        "\n",
        "**Step 5: Evaluate Performance**\n",
        "\n",
        "- Since it’s a disease prediction problem, accuracy alone isn’t enough.\n",
        "\n",
        "- I’d look at precision, recall, F1-score, and ROC-AUC to evaluate.\n",
        "\n",
        "- Recall is especially important here — missing a true disease case is costlier than a false alarm.\n",
        "\n",
        "**Business Value**\n",
        "\n",
        "- The model can help doctors screen patients faster, flagging high-risk individuals who may need urgent tests.\n",
        "\n",
        "- It can reduce misdiagnosis by acting as a decision support tool.\n",
        "\n",
        "- Saves costs for the hospital by focusing resources on the most at-risk patients.\n",
        "\n",
        "- For patients, it improves early detection, which often means better treatment outcomes and reduced severity of illness\n"
      ],
      "metadata": {
        "id": "jT-48Rxhs2Ly"
      }
    }
  ]
}